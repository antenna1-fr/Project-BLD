{
 "cells": [
  {
   "cell_type": "code",
   "id": "c1178e762b678273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:20:44.347511Z",
     "start_time": "2025-11-22T04:20:44.342263Z"
    }
   },
   "source": [
    "# =========================\n",
    "# XGB_Prototype_Modular.ipynb (Cell 1)\n",
    "# Modularized version using refactored components\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc, os, sys, math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping, EvaluationMonitor\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "from packaging import version\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# ---- project config ----\n",
    "config_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(config_path)\n",
    "import config as config\n",
    "\n",
    "# ---- Import modularized components ----\n",
    "from src.data.tabular_dataset import build_leak_proof_dataset\n",
    "from src.data.storage import DataStorage\n",
    "from src.backtest.splits import purged_time_splits, get_last_split\n",
    "from src.backtest.engine import run_backtest\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "4296b65f420218cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:20:46.205105Z",
     "start_time": "2025-11-22T04:20:46.202344Z"
    }
   },
   "source": [
    "# ---- paths ----\n",
    "DATASET_PATH = str(config.PROCESSED_DATA_PATH)\n",
    "OUTPUT_PLOT = str(config.XGB_CONFUSION_MATRIX_PLOT)\n",
    "OUTPUT_PREDICTIONS = str(config.PREDICTIONS_CSV)\n",
    "OUTPUT_SHAP = str(config.OUTPUTS_DIR / \"shap_feature_importance.csv\")\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "fc3a67cf2440956e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:24:49.287840Z",
     "start_time": "2025-11-22T04:20:47.992310Z"
    }
   },
   "source": [
    "# =========================\n",
    "# MODULAR DATA LOADING (Cell 3, v2 regression)\n",
    "# Uses DataStorage to load v2 execution-aware dataset\n",
    "# =========================\n",
    "\n",
    "print(f\"[modular] Loading data from {DATASET_PATH}\")\n",
    "\n",
    "storage = DataStorage(processed_path=DATASET_PATH)\n",
    "df_raw = storage.load_full_table()\n",
    "print(f\"[modular] Raw loaded: {len(df_raw):,} rows, {df_raw.shape[1]} columns\")\n",
    "\n",
    "# ---- basic cleaning / filters ----\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Require tradable row (as defined in data_preparer v2)\n",
    "if \"tradable\" in df.columns:\n",
    "    df = df[df[\"tradable\"] == 1].copy()\n",
    "\n",
    "# Require non-NaN execution-aware labels and non-zero label size\n",
    "target_cols = [\"y_long_best\", \"y_long_drawdown\", \"y_short_best\", \"y_short_drawup\"]\n",
    "for c in target_cols:\n",
    "    if c not in df.columns:\n",
    "        raise RuntimeError(f\"Expected column '{c}' not found in processed dataset.\")\n",
    "\n",
    "df = df[df[\"y_long_best\"].notna()].copy()\n",
    "if \"label_qty\" in df.columns:\n",
    "    df = df[df[\"label_qty\"] > 0].copy()\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Primary regression target (can change later)\n",
    "PRIMARY_TARGET = \"y_long_best\"\n",
    "AUX_TARGETS = [\"y_long_drawdown\", \"y_short_best\", \"y_short_drawup\"]\n",
    "\n",
    "# Identify columns that should NOT be used as features (targets, ids, legacy)\n",
    "exclude_exact = set(\n",
    "    [\"item\", \"timestamp\"] +\n",
    "    target_cols +\n",
    "    [\n",
    "        \"target_min_abs\", \"target_max_abs\", \"target_min_rel\", \"target_max_rel\",\n",
    "        \"target_q_up_abs\", \"target_q_dn_abs\", \"target_q_up_rel\", \"target_q_dn_rel\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Exclude any other columns starting with legacy target prefix\n",
    "exclude_prefixes = (\"target_\",)\n",
    "\n",
    "feature_cols = []\n",
    "for c in df.columns:\n",
    "    if c in exclude_exact:\n",
    "        continue\n",
    "    if any(c.startswith(pref) for pref in exclude_prefixes):\n",
    "        continue\n",
    "    # keep everything else as potential feature (including mid_price, px_entry_*, exec_spread_rel_labelQ, jump_flag, etc.)\n",
    "    feature_cols.append(c)\n",
    "\n",
    "# Target vector\n",
    "y = df[PRIMARY_TARGET].astype(\"float32\").to_numpy()\n",
    "\n",
    "# After df is finalized (df = df_raw[...] / df = df_raw.copy(); etc.)\n",
    "del df_raw\n",
    "gc.collect()\n",
    "print(\"[mem] Dropped df_raw\")\n",
    "\n",
    "print(f\"[modular] Dataset ready (v2 regression): {len(df):,} rows\")\n",
    "print(f\"[modular] Features: {len(feature_cols)}; primary target: {PRIMARY_TARGET}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modular] Loading data from C:\\Users\\reyno\\Documents\\GitHub\\Project-BLD\\data\\processed\\improved_normalized_labeled.parquet\n",
      "[modular] Raw loaded: 41,719,080 rows, 131 columns\n",
      "[mem] Dropped df_raw\n",
      "[modular] Dataset ready (v2 regression): 41,608,256 rows\n",
      "[modular] Features: 117; primary target: y_long_best\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "6019a6625ce917e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:26:01.900134Z",
     "start_time": "2025-11-22T04:25:47.366617Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Split using modularized splits module (Cell 4)\n",
    "# =========================\n",
    "print(\"[modular] Creating train/val split using purged_time_splits...\")\n",
    "\n",
    "# Use the modularized splitting function\n",
    "train_idx, val_idx = get_last_split(df, n_splits=5, embargo=0)\n",
    "\n",
    "# Extract features for training\n",
    "X = df[feature_cols]\n",
    "X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "w_tr = None  # no class weights for regression\n",
    "\n",
    "del X\n",
    "\n",
    "print(\"[mem] Dropped X\")\n",
    "\n",
    "print(f\"[modular] Train: {len(X_tr):,} | Val: {len(X_val):,}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modular] Creating train/val split using purged_time_splits...\n",
      "[modular] Train: 34,673,547 | Val: 6,934,709\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:32:04.912116Z",
     "start_time": "2025-11-22T04:32:04.027554Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "16e32155ec97b387",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c60e425d7803179a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:27:14.379324Z",
     "start_time": "2025-11-22T04:27:14.154634Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Leak check (Cell 5)\n",
    "# =========================\n",
    "print(\"[modular] Running leak check...\")\n",
    "sus_target = [c for c in df.columns if c.startswith(\"target_\")]\n",
    "sus_y = [c for c in df.columns if c.startswith(\"y_\")]\n",
    "print(f\"[sanity] legacy target_* columns present: {sus_target[:8]}{'...' if len(sus_target) > 8 else ''}\")\n",
    "print(f\"[sanity] y_* regression label columns present: {sus_y}\")\n",
    "\n",
    "# For reference: PRIMARY_TARGET is a continuous value now\n",
    "print(f\"[sanity] PRIMARY_TARGET: {PRIMARY_TARGET}, y mean={np.nanmean(y):.4f}, std={np.nanstd(y):.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modular] Running leak check...\n",
      "[sanity] legacy target_* columns present: ['target_min_abs', 'target_max_abs', 'target_min_rel', 'target_max_rel', 'target_q_up_abs', 'target_q_dn_abs', 'target_q_up_rel', 'target_q_dn_rel']\n",
      "[sanity] y_* regression label columns present: ['y_long_best', 'y_long_drawdown', 'y_short_best', 'y_short_drawup']\n",
      "[sanity] PRIMARY_TARGET: y_long_best, y mean=307.4051, std=14970.4609\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "977234e5b36c8bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:33:41.634426Z",
     "start_time": "2025-11-22T04:33:41.624405Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Training utilities (Cell 6)\n",
    "# =========================\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "USE_GPU = True\n",
    "USE_OPTUNA = False\n",
    "N_TRIALS = 10\n",
    "LOAD_EXISTING_MODEL = False\n",
    "SAVE_MODEL_AFTER_TRAIN = True\n",
    "OVERWRITE_SAVED_PARAMS = True\n",
    "\n",
    "MODEL_DIR = Path(str(config.OUTPUTS_DIR)) / \"xgb\"\n",
    "PARAMS_PATH = MODEL_DIR / \"xgb_best_params.json\"\n",
    "MODEL_PATH = MODEL_DIR / \"xgb_model.ubj\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import json\n",
    "\n",
    "def _save_json(obj, p: Path):\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def _load_json(p: Path):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _np_view(df):\n",
    "    return df.to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "def _merge_common(best: dict, use_gpu: bool) -> dict:\n",
    "    common = {\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": 3,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "        \"n_jobs\": max(1, os.cpu_count()-1),\n",
    "        \"early_stopping_rounds\": 100,\n",
    "        \"device\": (\"cuda\" if use_gpu else \"cpu\"),\n",
    "    }\n",
    "    merged = {**common, **best}\n",
    "    return merged\n",
    "\n",
    "def _merge_common_reg(best: dict, use_gpu: bool) -> dict:\n",
    "    common = {\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "        \"n_jobs\": max(1, os.cpu_count()-1),\n",
    "        \"early_stopping_rounds\": 100,\n",
    "        \"device\": (\"cuda\" if use_gpu else \"cpu\"),\n",
    "    }\n",
    "    merged = {**common, **best}\n",
    "    return merged\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "4cf388d9c7dc34c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:33:44.218194Z",
     "start_time": "2025-11-22T04:33:44.211940Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Opportunity coverage helper (Cell 7)\n",
    "# =========================\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "def _forward_extrema(df_item, horizon):\n",
    "    \"\"\"Compute forward max/min returns for a single item's time series.\"\"\"\n",
    "    x = df_item['mid_price'].to_numpy()\n",
    "    if len(x) < horizon:\n",
    "        out = df_item[['timestamp','item','mid_price']].copy()\n",
    "        out['fwd_up_ret'] = np.nan\n",
    "        out['fwd_dn_ret'] = np.nan\n",
    "        return out\n",
    "    win = sliding_window_view(x, horizon)\n",
    "    fwd_max = np.concatenate([win.max(1), np.full(horizon-1, np.nan)])\n",
    "    fwd_min = np.concatenate([win.min(1), np.full(horizon-1, np.nan)])\n",
    "    out = df_item[['timestamp','item','mid_price']].copy()\n",
    "    out['fwd_up_ret'] = (fwd_max - x) / x\n",
    "    out['fwd_dn_ret'] = (x - fwd_min) / x\n",
    "    return out\n",
    "\n",
    "def opportunity_coverage(prices, preds, horizon=60, up_tau=0.03, dn_tau=0.03, score_col='score'):\n",
    "    \"\"\"\n",
    "    Compute opportunity coverage metrics.\n",
    "\n",
    "    Measures how well the model captures good trading opportunities\n",
    "    within the specified horizon.\n",
    "    \"\"\"\n",
    "    preds = preds.copy()\n",
    "    if score_col not in preds:\n",
    "        preds['score'] = preds['Pp1'] - preds['Pm1']\n",
    "    prices = prices.sort_values(['item','timestamp']).copy()\n",
    "    opp = (prices.groupby('item', group_keys=False)\n",
    "                 .apply(lambda g: _forward_extrema(g, horizon)))\n",
    "\n",
    "    joined = preds.merge(opp, on=['item','timestamp'], how='inner')\n",
    "    joined['good_up']  = joined['fwd_up_ret'] >= up_tau\n",
    "    joined['good_dn']  = joined['fwd_dn_ret'] >= dn_tau\n",
    "    joined['flag_buy'] = joined['score'] > 0\n",
    "    joined['flag_sell']= joined['score'] < 0\n",
    "\n",
    "    metrics = {\n",
    "        'recall@good_up': (joined['flag_buy'] & joined['good_up']).sum() / max(1, joined['good_up'].sum()),\n",
    "        'precision@buy':  (joined['flag_buy'] & (joined['fwd_up_ret']>0)).sum() / max(1, joined['flag_buy'].sum()),\n",
    "        'recall@good_dn': (joined['flag_sell'] & joined['good_dn']).sum() / max(1, joined['good_dn'].sum()),\n",
    "        'precision@sell': (joined['flag_sell'] & (joined['fwd_dn_ret']>0)).sum() / max(1, joined['flag_sell'].sum()),\n",
    "        'coverage%':      100 * joined.groupby('timestamp')['flag_buy'].max().mean()\n",
    "    }\n",
    "    return pd.Series(metrics), joined\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "73d40bddb27ccef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:37:13.481004Z",
     "start_time": "2025-11-22T04:33:47.648607Z"
    }
   },
   "source": [
    "\n",
    "# =========================\n",
    "# Training function (Cell 8, v2 regression)\n",
    "# =========================\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def train_regressor(\n",
    "        X_tr, y_tr, X_val, y_val,\n",
    "        w_tr=None,\n",
    "        use_gpu=USE_GPU,\n",
    "        params_path=PARAMS_PATH,\n",
    "        model_path=MODEL_PATH,\n",
    "        load_existing_model=LOAD_EXISTING_MODEL,\n",
    "        save_model_after_train=SAVE_MODEL_AFTER_TRAIN,\n",
    "        overwrite_saved_params=OVERWRITE_SAVED_PARAMS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train XGBoost regressor on PRIMARY_TARGET (e.g. y_long_best).\n",
    "    \"\"\"\n",
    "\n",
    "    # Fast path: load existing model\n",
    "    if load_existing_model and Path(model_path).exists():\n",
    "        print(f\"[train] Loading existing regressor → {model_path}\")\n",
    "        reg = xgb.XGBRegressor()\n",
    "        reg.load_model(str(model_path))\n",
    "        if Path(params_path).exists():\n",
    "            params = _load_json(params_path)\n",
    "        else:\n",
    "            params = {\"framework\": \"xgb_reg\", \"loaded_from_model\": True}\n",
    "        return reg, params\n",
    "\n",
    "    # Load saved params if present\n",
    "    loaded_params = None\n",
    "    if Path(params_path).exists():\n",
    "        loaded_raw = _load_json(params_path)\n",
    "        if loaded_raw.get(\"framework\") == \"xgb_reg\":\n",
    "            # Strip framework tag; keep only true XGB params\n",
    "            loaded_params = {k: v for k, v in loaded_raw.items() if k != \"framework\"}\n",
    "            print(f\"[train] Loaded saved reg params from {params_path}\")\n",
    "        else:\n",
    "            print(f\"[train] Found non-regression params at {params_path} (framework={loaded_raw.get('framework')}) – ignoring for regressor.\")\n",
    "\n",
    "\n",
    "    # NumPy views\n",
    "    X_tr_np = _np_view(X_tr)\n",
    "    X_val_np = _np_view(X_val)\n",
    "\n",
    "    # Use loaded params or baseline\n",
    "    if loaded_params is not None:\n",
    "        best = loaded_params\n",
    "    else:\n",
    "        print(\"[train] No reg params file found — using baseline reg params.\")\n",
    "        best = {\n",
    "            \"n_estimators\": 650,\n",
    "            \"max_depth\": 12,\n",
    "            \"learning_rate\": 0.06,\n",
    "            \"subsample\": 0.85,\n",
    "            \"colsample_bytree\": 0.85,\n",
    "            \"min_child_weight\": 2.0,\n",
    "            \"reg_lambda\": 1.0,\n",
    "            \"reg_alpha\": 0.0,\n",
    "            \"gamma\": 0.0,\n",
    "            \"max_bin\": 256,\n",
    "            \"grow_policy\": \"depthwise\",\n",
    "        }\n",
    "    best = _merge_common_reg(best, use_gpu)\n",
    "\n",
    "    print(best)\n",
    "\n",
    "    # Train regressor\n",
    "    reg = xgb.XGBRegressor(**best)\n",
    "    reg.fit(\n",
    "        X_tr_np,\n",
    "        y_tr,\n",
    "        sample_weight=w_tr,\n",
    "        eval_set=[(X_val_np, y_val)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Basic validation metrics\n",
    "    y_val_pred = reg.predict(X_val_np)\n",
    "    mse = mean_squared_error(y_val, y_val_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    print(f\"[train] Val RMSE: {rmse:.5f}, R^2: {r2:.4f}\")\n",
    "\n",
    "\n",
    "    if save_model_after_train:\n",
    "        try:\n",
    "            reg.save_model(str(model_path))\n",
    "            print(f\"[train] Saved regressor → {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[train][warn] Could not save model: {e}\")\n",
    "    try:\n",
    "        best_out = {\"framework\": \"xgb_reg\", **best}\n",
    "        if overwrite_saved_params:\n",
    "            _save_json(best_out, params_path)\n",
    "            print(f\"[train] Saved reg params → {params_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[train][warn] Could not save params: {e}\")\n",
    "\n",
    "    return reg, {\"framework\": \"xgb_reg\", **best}\n",
    "\n",
    "\n",
    "# ---- Run training ----\n",
    "print(\"[modular] Training regressor on\", PRIMARY_TARGET, \"...\")\n",
    "reg, params = train_regressor(\n",
    "    X_tr, y_tr, X_val, y_val,\n",
    "    w_tr=w_tr,\n",
    "    use_gpu=USE_GPU,\n",
    ")\n",
    "print(\"[modular] Regressor params:\", params)\n",
    "gc.collect()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modular] Training regressor on y_long_best ...\n",
      "[train] No reg params file found — using baseline reg params.\n",
      "{'tree_method': 'hist', 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42, 'n_jobs': 31, 'early_stopping_rounds': 100, 'device': 'cuda', 'n_estimators': 650, 'max_depth': 12, 'learning_rate': 0.06, 'subsample': 0.85, 'colsample_bytree': 0.85, 'min_child_weight': 2.0, 'reg_lambda': 1.0, 'reg_alpha': 0.0, 'gamma': 0.0, 'max_bin': 256, 'grow_policy': 'depthwise'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reyno\\Documents\\GitHub\\Project-BLD\\venv311\\Lib\\site-packages\\xgboost\\core.py:729: UserWarning: [22:37:07] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Val RMSE: 4103.26602, R^2: 0.7306\n",
      "[train] Saved regressor → C:\\Users\\reyno\\Documents\\GitHub\\Project-BLD\\outputs\\xgb\\xgb_model.ubj\n",
      "[train] Saved reg params → C:\\Users\\reyno\\Documents\\GitHub\\Project-BLD\\outputs\\xgb\\xgb_best_params.json\n",
      "[modular] Regressor params: {'framework': 'xgb_reg', 'tree_method': 'hist', 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42, 'n_jobs': 31, 'early_stopping_rounds': 100, 'device': 'cuda', 'n_estimators': 650, 'max_depth': 12, 'learning_rate': 0.06, 'subsample': 0.85, 'colsample_bytree': 0.85, 'min_child_weight': 2.0, 'reg_lambda': 1.0, 'reg_alpha': 0.0, 'gamma': 0.0, 'max_bin': 256, 'grow_policy': 'depthwise'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "814"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "7c8e244ebca6900b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T05:22:15.779317Z",
     "start_time": "2025-11-22T05:21:56.782152Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Regression outputs: preds_test, metrics, backtest CSV\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "from src.analysis.directional_eval import run_regression_outputs\n",
    "\n",
    "PRIMARY_TARGET = PRIMARY_TARGET  # already defined earlier in your notebook\n",
    "\n",
    "res = run_regression_outputs(\n",
    "    df=df,\n",
    "    val_idx=val_idx,\n",
    "    reg=reg,\n",
    "    X_val=X_val_np if \"X_val_np\" in globals() else _np_view(X_val),\n",
    "    y_val=y_val,\n",
    "    primary_target=PRIMARY_TARGET,\n",
    "    output_predictions=OUTPUT_PREDICTIONS,\n",
    "    vol_est_col=\"vol_est\",\n",
    "    k_softmax=20.0,\n",
    "    up_tau_reg=0.02,\n",
    "    dn_tau_reg=0.02,\n",
    ")\n",
    "\n",
    "y_val_pred = res[\"y_val_pred\"]\n",
    "preds_test = res[\"preds_test\"]\n",
    "reg_metrics = res[\"metrics\"]\n",
    "pred_df = res[\"pred_df\"]\n",
    "\n",
    "print(\"[modular] Regression metrics on val:\")\n",
    "print(f\"  RMSE: {reg_metrics['rmse']:.6f}\")\n",
    "print(f\"  R^2:  {reg_metrics['r2']:.4f}\")\n",
    "print(f\"[modular] Saved regression-based predictions → {res['output_path']}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reyno\\Documents\\GitHub\\Project-BLD\\src\\analysis\\directional_eval.py:161: RuntimeWarning: overflow encountered in exp\n",
      "  pred_proba_buy = 1.0 / (1.0 + np.exp(-k * y_pred_cont))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modular] Regression metrics on val:\n",
      "  RMSE: 4103.266016\n",
      "  R^2:  0.7306\n",
      "[modular] Saved regression-based predictions → C:\\Users\\reyno\\Documents\\GitHub\\Project-BLD\\outputs\\xgb_predictions.csv\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "eb324945839b6041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T05:30:56.406897Z",
     "start_time": "2025-11-22T05:22:38.295850Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Backtest tuning + best backtest (Cell 15 alt)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "from src.backtest.engine import tune_backtest_knobs\n",
    "\n",
    "print(\"[modular] Tuning backtest knobs with src.backtest.engine...\")\n",
    "\n",
    "TRADE_LOG_PATH = Path(config.XGB_TRADE_LOG_CSV)\n",
    "PLOTS_DIR = Path(config.XGB_TRADING_DIR)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TRADE_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run tuning on the same predictions CSV you already generate\n",
    "study, best_summary, best_trades, best_eq = tune_backtest_knobs(\n",
    "    pred_path=Path(OUTPUT_PREDICTIONS),   # uses the CSV you created earlier\n",
    "    study_name=\"bt_knobs_v23\",\n",
    "    storage_dir=\"backtests/db\",\n",
    "    total_trials=200,                     # adjust as needed\n",
    "    trade_log_path=TRADE_LOG_PATH,       # also saves best trades to CSV\n",
    "    initial_capital=1_000_000_000.0,\n",
    "    max_trades_per_minute=2,\n",
    "    spread_bps=10.0,\n",
    "    fee_bps=100.0,\n",
    "    slippage_bps=0.0,\n",
    "    median_window=30,\n",
    "    impact_cap_bps=200.0,\n",
    "    max_positions_per_item=1,\n",
    "    cooldown_minutes=0,\n",
    "    bar_seconds=60,\n",
    ")\n",
    "\n",
    "# Make results visible to the diagnostics cell\n",
    "trades = best_trades\n",
    "equity = best_eq\n",
    "summary = best_summary\n",
    "\n",
    "print(\"[modular] Best backtest summary:\")\n",
    "for k, v in best_summary.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"[modular] Best trades saved to: {TRADE_LOG_PATH}\")\n",
    "print(f\"[modular] Optuna best params: {study.best_params}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modular] Tuning backtest knobs with src.backtest.engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 23:22:41,474] A new study created in RDB with name: bt_knobs_v23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[optuna] completed trials: 200\n",
      "Best params: {'min_trade_amount': 8545694.214414444, 'min_confidence': 0.9369703773088872, 'exit_profit_threshold': 0.34492252499058945, 'stop_loss_threshold': 0.11887324842035293, 'persist_bars': 26, 'alpha': 2.6951118763957527, 'min_confidence_streak': 0.5944538757487808}\n",
      "Best value: 1.326528693516282\n",
      "Summary: {'Final Capital': '1,643,716,285', 'Total Profit': '643,716,285', 'Num Trades': 1321, 'Win Rate': '31.42%', 'Average Return / Trade': '2.17%', 'Average Win': '32.06%', 'Average Loss': '-11.52%', 'Gross Profit': '3,776,546,253', 'Gross Loss': '-3,132,829,968', 'Profit Factor': '1.21', 'Average Duration (min)': '2172.97'}\n",
      "[modular] Best backtest summary:\n",
      "  Final Capital: 1,643,716,285\n",
      "  Total Profit: 643,716,285\n",
      "  Num Trades: 1321\n",
      "  Win Rate: 31.42%\n",
      "  Average Return / Trade: 2.17%\n",
      "  Average Win: 32.06%\n",
      "  Average Loss: -11.52%\n",
      "  Gross Profit: 3,776,546,253\n",
      "  Gross Loss: -3,132,829,968\n",
      "  Profit Factor: 1.21\n",
      "  Average Duration (min): 2172.97\n",
      "[modular] Best trades saved to: C:\\Users\\reyno\\Documents\\GitHub\\Project-BLD\\outputs\\trading\\XGB_trading\\XGB_trade_log.csv\n",
      "[modular] Optuna best params: {'min_trade_amount': 8545694.214414444, 'min_confidence': 0.9369703773088872, 'exit_profit_threshold': 0.34492252499058945, 'stop_loss_threshold': 0.11887324842035293, 'persist_bars': 26, 'alpha': 2.6951118763957527, 'min_confidence_streak': 0.5944538757487808}\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "bbf50b1031d1843a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T20:04:45.595652Z",
     "start_time": "2025-11-29T20:04:45.459395Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Diagnostic plots (Cell 16) — using run_discrete_backtest_diagnostics\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path\n",
    "from src.analysis.discrete_diagnostics import run_discrete_backtest_diagnostics\n",
    "\n",
    "print(f\"[modular] Saving plots to: {PLOTS_DIR}\")\n",
    "\n",
    "# Build kwargs defensively so the cell doesn't explode if some vars are missing\n",
    "diag_kwargs = {\n",
    "    \"trades\": locals().get(\"trades\", None),\n",
    "    \"equity\": locals().get(\"equity\", None),\n",
    "    \"clf\": locals().get(\"clf\", None),\n",
    "    \"X_val\": locals().get(\"X_val\", None),\n",
    "    \"y_val\": locals().get(\"y_val\", None),\n",
    "    \"proba_val\": locals().get(\"proba_val\", None),\n",
    "    \"classes_dec\": locals().get(\"classes_dec\", None),\n",
    "    \"opp_joined\": locals().get(\"opp_joined\", None),\n",
    "    \"preds_test\": locals().get(\"preds_test\", None),\n",
    "    \"plots_dir\": PLOTS_DIR,  # uses the same directory as before\n",
    "}\n",
    "\n",
    "saved_paths = run_discrete_backtest_diagnostics(**diag_kwargs)\n",
    "\n",
    "print(\"[modular] Done. Generated plots:\")\n",
    "for name, path in saved_paths.items():\n",
    "    print(f\"  {name}: {path}\")\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.analysis.discrete_diagnostics'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# =========================\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# Diagnostic plots (Cell 16) — using run_discrete_backtest_diagnostics\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# =========================\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpathlib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msrc\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01manalysis\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdiscrete_diagnostics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m run_discrete_backtest_diagnostics\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[modular] Saving plots to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPLOTS_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# Build kwargs defensively so the cell doesn't explode if some vars are missing\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'src.analysis.discrete_diagnostics'"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
